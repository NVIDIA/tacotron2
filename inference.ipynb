{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tacotron 2 inference code \n",
    "Edit the variables **checkpoint_path** and **text** to match yours and run the entire code to generate plots of mel outputs, alignments and audio synthesis from the generated mel-spectrogram using Griffin-Lim."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import libraries and setup matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pylab as plt\n",
    "import IPython.display as ipd\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from hparams import create_hparams\n",
    "from model import Tacotron2\n",
    "from layers import TacotronSTFT\n",
    "from audio_processing import griffin_lim\n",
    "from train import load_model\n",
    "from text import text_to_sequence\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(data, figsize=(16, 4)):\n",
    "    fig, axes = plt.subplots(1, len(data), figsize=figsize)\n",
    "    for i in range(len(data)):\n",
    "        axes[i].imshow(data[i], aspect='auto', origin='bottom', \n",
    "                       interpolation='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Parsing command line hparams: distributed_run=0,mask_padding=0\n"
     ]
    }
   ],
   "source": [
    "hparams = create_hparams(\"distributed_run=0,mask_padding=0\")\n",
    "hparams.sampling_rate = 22050\n",
    "hparams.filter_length = 1024\n",
    "hparams.hop_length = 256\n",
    "hparams.win_length = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load model from checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hendorf/code/audiodrama/project/tacotron2/layers.py:35: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  self.conv.weight, gain=torch.nn.init.calculate_gain(w_init_gain))\n",
      "/Users/hendorf/code/audiodrama/project/tacotron2/layers.py:15: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  gain=torch.nn.init.calculate_gain(w_init_gain))\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = \"/Users/hendorf/code/audiodrama/project/tacotron2/outdir/checkpoint_45500\"\n",
    "model = load_model(hparams)\n",
    "try:\n",
    "    model = model.module\n",
    "except:\n",
    "    pass\n",
    "model.load_state_dict({k.replace('module.',''):v for k,v in torch.load(checkpoint_path)['state_dict'].items()})\n",
    "_ = model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare text input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Once upon a time there was a little mermaid named Siren, who lived with her step mother under the sea, She didn't get to go out of the Sea like any other.\"\n",
    "#text = \"Johannes \"*10\n",
    "#text = \"Willkomen bei dem BBQ, es gibt hier Bratwürste und Käseschnitzel in der Lounge. Melden Sie sich hier an.\"\n",
    "#text = \"Welcome to the Big Data Barbeque, we are happy we have some great speakers from all over Europe here.\"\n",
    "#text = \"Once upon a time in a splendid palace on the bed of the bluest ocean, lived the Sea King, a wise old triton with a long flowing white beard. \"\n",
    "#text = \"He lived in a magnificent palace, built of gaily coloured coral and seashells, together with his five daughters, very beautiful mermaids. Sirenetta, the youngest and loveliest of them all, also had a beautiful voice, and when she sang, the fishes flocked from all over the sea to listen to her. The shells gaped wide, showing their pearls and even the jellyfish stopped to listen. The young mermaid often sang, and each time, she would gaze upwards, seeking the faint sunlight that scarcely managed to filter down into the depths. Oh, how I'd love to go up there and at last see the sky, which everyone says is so pretty, and hear the voices of humans and smell the scent of the flowers! 'You're still too young!' said her mother. 'In a year or two, when you\\'re fifteen. Only then will the King let you go up there, like your sisters!' Sirenetta spent her time wishing for the world of humans, she listened to her sisters' stories, and every time they returned frorm the surface, she would ask them questions, to satisfy her curiosity.\"\n",
    "sequence = np.array(text_to_sequence(text, ['english_cleaners']))[None, :]\n",
    "sequence = torch.autograd.Variable(\n",
    "    torch.from_numpy(sequence)).cuda().long()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decode text input and plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mel_outputs, mel_outputs_postnet, _, alignments = model.inference(sequence)\n",
    "plot_data((mel_outputs.data.cpu().numpy()[0],\n",
    "           mel_outputs_postnet.data.cpu().numpy()[0],\n",
    "           alignments.data.cpu().numpy()[0].T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load TacotronSTFT and convert mel-spectrogram to spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taco_stft = TacotronSTFT(\n",
    "    hparams.filter_length, hparams.hop_length, hparams.win_length, \n",
    "    sampling_rate=hparams.sampling_rate)\n",
    "mel_decompress = taco_stft.spectral_de_normalize(mel_outputs_postnet)\n",
    "mel_decompress = mel_decompress.transpose(1, 2).data.cpu()\n",
    "spec_from_mel_scaling = 1000\n",
    "spec_from_mel = torch.mm(mel_decompress[0], taco_stft.mel_basis)\n",
    "spec_from_mel = spec_from_mel.transpose(0, 1).unsqueeze(0)\n",
    "spec_from_mel = spec_from_mel * spec_from_mel_scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Synthesize audio from spectrogram using the Griffin-Lim algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waveform = griffin_lim(torch.autograd.Variable(spec_from_mel[:, :, :-1]), \n",
    "                       taco_stft.stft_fn, 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(waveform[0].data.cpu().numpy(), rate=hparams.sampling_rate) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Once upon a time there was a little mermaid named Siren, who lived with her step mother under the sea, She didn't get to go out of the Sea like any other.\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "librosa.output.write_wav(\n",
    "    '/Users/hendorf/Downloads/onceuponatime_445K.wav', \n",
    "    waveform[0].data.cpu().numpy(), \n",
    "    hparams.sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_CudaDeviceProperties(name='GeForce GTX 1080', major=6, minor=1, total_memory=8191MB, multi_processor_count=20)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_properties(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
